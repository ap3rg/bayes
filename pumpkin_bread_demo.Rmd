---
title: "Rest and Digest"
author: "Andrea Parra & Elvira Fleury"
date: "2025-11-10"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(ggplot2)
```

## Introduction 

Elvira baked a pumpkin bread last night, and brought it in to share with the class. She seems very earnest, but you aren't sure if it's any good. You're pretty full from eating lunch, so there isn't much room for pumpkin bread! Your goal is to eat the cake only if there’s at least a 70% chance that it will turn out well — in other words, if the recipe’s success rate is at least 0.7.

Here are some things that we know about the pumpkin bread: 

Elvira got the recipe from Andrea. Elvira has baked it 10 times so far, with somewhat inconsistent success. It has been smashing hit 5 times, but gone straight into the compost bin the other 5 times.

This following section is adapted from the blog post, "The Beta-Binomial Model: An Introduction to Bayesian Statistics" by Rodrigo Morales. All code is adapted from Kevin Ross' "STAT415 Handouts". To help us decide whether to eat the cake, we are interested in estimating the probability that the recipe produces a good result, denoted by $\theta$.

As in the example with the beans, we can think of each cake baking session outcome as a Bernoulli random variable (either it is good or not) with probability of success $\theta$. However, our data has results from more than one session. Assuming that Elvira doesn't get better each time she bakes (:( ) and that each baking session is independent from the other, the random variable representing the number of delicious cakes among (y) the baking sessions (n) is binomially distributed. 

```{r}
# data
n =  # Add here
y =  # Add here 
```


To finish the model specification, we need to define a prior distribution on $\theta$. As discussed previously, the beta function is a good choice here, since it provides both flexibility and mathematical tractability.   

Now we are ready to derive the posterior distribution using Bayes’ theorem. As we saw in the bean example, the posterior density is itself a beta distribution, and the posterior mean is a weighted average between the data rate (y/n) and the prior mean ($\alpha$/k), where the weights are determined by n and k. If n>k, the data will have a larger weight than the prior, if the converse is true, the weight of the prior will be larger.


Depending on the group you were assigned, please proceed to your section. 
Do not read the other group's section, or you will ruin all the fun! 

After you complete your section, return here and answer the following questions. We will discuss them as a group.  

1) What do the shape and position of the posterior and likelihood densities tell us about the agreement between a frequentist and Bayesian approach?

2) What is the posterior expected success rate for the recipe? Does this agree with the other group's answer? Why or why not?

3) What is the 95% credible interval? Interpret this interval in plain language. How does this interpretation differ from the frequentist interpretation of a 95% confidence interval. 

4) What is the posterior probability that the recipe success rate is greater or equal to 0.7? Could you answer this if you were adopting a frequentist approach?

5) Given that Andrea is a Michelin-starred chef, how do we feel about including Andrea's record as prior information when Elvira is the one baking? What are the benefits and drawbacks of doing so?

6) How do you feel about eating the cake?


\newpage

## Group 1:

You don't get any more information :P That means you will be adopting an uninformative
prior. 

```{r, echo = F}
# Do not edit 
# Define colors and line types for Bayesian plots
bayes_col <- c(
  prior = "#1b9e77",
  likelihood = "#7570b3",
  posterior = "#d95f02"
)

bayes_lty <- c(
  prior = "dotted",
  likelihood = "dotdash",
  posterior = "solid"
)

```


```{r}
#  Uninformative Beta prior
alpha_prior = # Add here 
beta_prior = # Add here 

# Beta posterior
alpha_post = # Add here 
beta_post = # Add here 
```

```{r}
### DO NOT EDIT ###

## The scaling is simply for visualization purposes

# scaled likelihood, depends on data: n, y this allows it to be on same scale
likelihood_scaled <- function(theta) {
  likelihood <- function(theta) {
    dbinom(x = y, size = n, prob = theta)
  }
  scaling_constant <- integrate(likelihood, lower = 0, upper = 1)[[1]]
  likelihood(theta) / scaling_constant
}

# Plot
ggplot(data.frame(x = c(0, 1)),
       aes(x = x)) +
  # prior
  stat_function(fun = dbeta,
                args = list(shape1 = alpha_prior,
                            shape2 = beta_prior),
                lty = bayes_lty["prior"],
                linewidth = 1,
                aes(color = "prior", linetype = "prior")) +
  # posterior
  stat_function(fun = dbeta,
                args = list(shape1 = alpha_post,
                            shape2 = beta_post),
                lty = bayes_lty["posterior"],
                linewidth = 1,
                aes(color = "posterior", linetype = "posterior")) +
  
  # (scaled) likelihood
  stat_function(fun = likelihood_scaled,
                lty = bayes_lty["likelihood"],
                linewidth = 1,
                aes(color = "likelihood", linetype = "likelihood")) +
  # Define color and add a legend
  scale_color_manual(name = "",
                     breaks = c("prior", "likelihood", "posterior"),
                     values = bayes_col[c("prior", "likelihood", "posterior")]) +
  scale_linetype_manual(name = "",
                        breaks = c("prior", "likelihood", "posterior"),
                        values = bayes_lty[c("prior", "likelihood", "posterior")]) +
  labs(x = "theta",
       y = "") +
  theme_bw()
```




```{r, echo = F}
## D0 NOT EDIT 
theta <- seq(0, 1, length.out = 1000)
posterior <- dbeta(theta, alpha_post, beta_post)

ggplot(data.frame(theta, posterior), aes(x = theta, y = posterior)) +
  geom_area(data = subset(data.frame(theta, posterior), theta > qbeta(0.025, alpha_post, beta_post) & theta < qbeta(0.975, alpha_post, beta_post)),
            aes(x = theta, y = posterior),
            fill = "skyblue", alpha = 0.5) +
  geom_line(color = "#d95f02", size = 1.2) +
  labs(x = expression(theta),
       y = "Posterior density",
       title = "95% Credible Interval") +
  theme_bw()
```


```{r, echo = F}
grp_1_res<- data.frame(
  est = round(alpha_post/ (alpha_post + beta_post),2),
  cred_int_lower = # FILL HERE # ADD COMMA AFTER 
  cred_int_upper = # FILL HERE # ADD COMMA AFTER 
  seventy_pct = round(1- pbeta(0.7, alpha_post, beta_post),2)
)

grp_1_res
```


\newpage

## Group 2:

You are really good friends with Andrea, and she lets you in on her secret: before coming to the school of public health she was a three-Michelin star chef. She created the recipe and has baked it one HUNDRED times. Of those times, it was only gross five times. Now, you have some PRIOR information :) 

```{r}
#  Informative Beta prior
alpha_prior = # FILL HERE 
beta_prior = # FILL HERE 

# Beta posterior
alpha_post = # FILL HERE 
beta_post =  # FILL HERE 
  
``` 

```{r}
### DO NOT EDIT ###

## The scaling is simply for visualization purposes

# scaled likelihood, depends on data: n, y this allows it to be on same scale
likelihood_scaled <- function(theta) {
  likelihood <- function(theta) {
    dbinom(x = y, size = n, prob = theta)
  }
  scaling_constant <- integrate(likelihood, lower = 0, upper = 1)[[1]]
  likelihood(theta) / scaling_constant
}

# Plot
ggplot(data.frame(x = c(0, 1)),
       aes(x = x)) +
  # prior
  stat_function(fun = dbeta,
                args = list(shape1 = alpha_prior,
                            shape2 = beta_prior),
                lty = bayes_lty["prior"],
                linewidth = 1,
                aes(color = "prior", linetype = "prior")) +
  # posterior
  stat_function(fun = dbeta,
                args = list(shape1 = alpha_post,
                            shape2 = beta_post),
                lty = bayes_lty["posterior"],
                linewidth = 1,
                aes(color = "posterior", linetype = "posterior")) +
  
  # (scaled) likelihood
  stat_function(fun = likelihood_scaled,
                lty = bayes_lty["likelihood"],
                linewidth = 1,
                aes(color = "likelihood", linetype = "likelihood")) +
  # Define color and add a legend
  scale_color_manual(name = "",
                     breaks = c("prior", "likelihood", "posterior"),
                     values = bayes_col[c("prior", "likelihood", "posterior")]) +
  scale_linetype_manual(name = "",
                        breaks = c("prior", "likelihood", "posterior"),
                        values = bayes_lty[c("prior", "likelihood", "posterior")]) +
  labs(x = "theta",
       y = "") +
  theme_bw()
```

```{r}

### DO NOT EDIT ###

theta <- seq(0, 1, length.out = 1000)
posterior <- dbeta(theta, alpha_post, beta_post)

ggplot(data.frame(theta, posterior), aes(x = theta, y = posterior)) +
  geom_area(data = subset(data.frame(theta, posterior), theta > qbeta(0.025, alpha_post, beta_post) & theta < qbeta(0.975, alpha_post, beta_post)),
            aes(x = theta, y = posterior),
            fill = "skyblue", alpha = 0.5) +
  geom_line(color = "#d95f02", size = 1.2) +
  labs(x = expression(theta),
       y = "Posterior density",
       title = "95% Credible Interval") +
  theme_bw()
```


```{r}
grp_2_res<- data.frame(
  est = round(alpha_post/ (alpha_post + beta_post),2),
  cred_int_lower = # FILL HERE # ADD A COMMA AFTER
  cred_int_upper = # FILL HERE # ADD A COMMA AFTER 
  seventy_pct = 1- pbeta(0.7, alpha_post, beta_post) # This is analogous to 
  #sampling from the posterior if you did not know exactly what it was 
)

grp_2_res
```


